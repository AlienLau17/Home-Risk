{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import csv, sys, time\n",
    "import sys\n",
    "import codecs\n",
    "from pyspark.sql import SparkSession, SQLContext, Row\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCESS_KEY = \"AKIAZ3SCTMVTH72HUI4M\"\n",
    "SECRET_KEY = \"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\"\n",
    "TOKEN = \"aa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"spark_json\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv():\n",
    "    objList = []\n",
    "    titleList = [\"business\", \"entertainment\", \"general\", \"health\", \"science\", \"sport\", \"technology\"]\n",
    "    bucket = \"wildbuckets-scrapy-finance\"\n",
    "    file_name = \"largeStream/2019_\"\n",
    "    s3 = boto3.client(\"s3\",\n",
    "        aws_access_key_id = ACCESS_KEY,\n",
    "        aws_secret_access_key = SECRET_KEY)\n",
    "    for title in titleList:\n",
    "        obj = s3.get_object(Bucket = bucket, Key = file_name + title + \".csv\")\n",
    "        objList.append(obj)\n",
    "    print(\"*******************\")\n",
    "    print(\"s3: \")\n",
    "    return objList\n",
    "\n",
    "# read from csv file in S3\n",
    "def get_csv_category(category):\n",
    "    bucket = \"wildbuckets-scrapy-finance\"\n",
    "    file_name = \"largeStream/2019_\"+category+\".csv\"\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        aws_access_key_id = ACCESS_KEY,\n",
    "        aws_secret_access_key = SECRET_KEY)\n",
    "    obj = s3.get_object(Bucket = bucket, Key = file_name)\n",
    "    body = obj['Body']\n",
    "    return body\n",
    "\n",
    "# the response of S3 bucket file is streamingBody\n",
    "# read from streamingBody, paralize the file into rdd\n",
    "def read_from_s3Body(category):\n",
    "    body = get_csv_category(category)\n",
    "    bodyList = []\n",
    "    i = 0\n",
    "#     print(type(body))\n",
    "    for ln in codecs.getreader('utf-8')(body):\n",
    "        \n",
    "        temp = []\n",
    "#         temp.append(category)\n",
    "        bodyTranslateList = ln.split(\",\")\n",
    "# #         print(ln)\n",
    "        \n",
    "#         line=shlex.shlex(ln)\n",
    "#         line.whitespace=','\n",
    "#         line.whitespace_split=True\n",
    "#         if i >10:\n",
    "#             print(ln)\n",
    "#         bodyTranslateList=list(line)\n",
    "#         print(bodyTranslateList)\n",
    "        \n",
    "        temp += bodyTranslateList\n",
    "        bodyList.append(temp)        \n",
    "    re = sc.parallelize(bodyList)\n",
    "    # print(\"re: \", re.show())\n",
    "#     print(\"Type of rdd: \", type(re))  \n",
    "    return re\n",
    "\n",
    "def convertFromRddtoDF(category):\n",
    "    spark = SparkSession.builder.appName(\"RDD_and_DataFrame\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "    rdd = read_from_s3Body(category)\n",
    "    row_rdd = rdd.map(lambda x: Row(x))\n",
    "    sqlContext = SQLContext(sc)\n",
    "    df=sqlContext.createDataFrame(row_rdd,['numbers'])\n",
    "    pandas_df = df.toPandas()\n",
    "#     print(pandas_df)\n",
    "    return pandas_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DF_reshape(df):\n",
    "    dict_list = []\n",
    "    for i in range(1, len(df)):\n",
    "        line = df.loc[i]['numbers']\n",
    "        l = len(line)\n",
    "        temp_dict = {}\n",
    "        temp_dict['url'] = line[0]\n",
    "        temp_dict['date'] = line[-1]\n",
    "        for j in range(1, l-1):\n",
    "            item = line[j]\n",
    "            if j == 1:\n",
    "                content = item\n",
    "            else:\n",
    "                content += item\n",
    "        temp_dict['content'] = content\n",
    "        dict_list.append(temp_dict)\n",
    "        file = pd.DataFrame(dict_list)\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(file):\n",
    "    url = file['url']\n",
    "    return url\n",
    "def get_desc(file):\n",
    "    desc = file['content']\n",
    "    return desc\n",
    "def get_date(file):\n",
    "    url_date = file['date']\n",
    "    return url_date\n",
    "def get_url_index(file):\n",
    "    url_list = get_url(file)\n",
    "    index = {}\n",
    "    v = 0\n",
    "    for u in url_list:\n",
    "        index[u] = v\n",
    "        v+=1\n",
    "    return index\n",
    "def get_filtered_list(text, split = True, word = False):\n",
    "    pattern1 = '[^\\D]'# all non-number\n",
    "    pattern2 = '[^\\w\\s]'\n",
    "    col = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    for s in text:\n",
    "        s = str(s).lower()\n",
    "        s = re.sub(pattern1, '', s)\n",
    "        s = re.sub(pattern2, '', s)\n",
    "        \n",
    "        if split:\n",
    "            s_splited = s.split()     \n",
    "            filtered_sentence = [w for w in s_splited if not w in stop_words] \n",
    "            if word == False:\n",
    "                col.append(filtered_sentence)\n",
    "            else:\n",
    "                for j in filtered_sentence:\n",
    "                    col.append(j)\n",
    "        else:\n",
    "            col.append(s)\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "def find_similar_url(tfidf_matrix, index_matrix, url_list, url, top_n = 10):\n",
    "    index = index_matrix[url]\n",
    "    cosine_similarities = linear_kernel(tfidf_matrix[index:index+1], tfidf_matrix).flatten()\n",
    "    related_docs_indices = [i for i in cosine_similarities.argsort()[::-1] if i != index]\n",
    "    return [url_list[index] for index in related_docs_indices][0:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_recommend_news(url_list, file, index):\n",
    "#     u = get_url(file)\n",
    "#     d = get_desc(file)\n",
    "#     row = []\n",
    "#     for i in url_list:\n",
    "#         line = {}\n",
    "# #         print(i)\n",
    "#         loc = int(index[i])\n",
    "# #         print(loc)\n",
    "#         url = u[loc]\n",
    "#         desp = d[loc]\n",
    "#         line['url'] = url\n",
    "#         line['description'] = desp\n",
    "#         row.append(line)\n",
    "#     return row\n",
    "def get_recommend_news(url_list, file, index):\n",
    "    u = get_url(file)\n",
    "    d = get_desc(file)\n",
    "    row = []\n",
    "    for i in url_list:\n",
    "        line = {}\n",
    "#         print(i)\n",
    "        loc = int(index[i])\n",
    "#         print(loc)\n",
    "        url = u[loc]\n",
    "        desp = d[loc]\n",
    "        line['url'] = url\n",
    "        line['description'] = desp\n",
    "        row.append(line)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendation_from_url(category, url):\n",
    "    df = convertFromRddtoDF(category)\n",
    "    file =DF_reshape(df)\n",
    "    description = get_desc(file)\n",
    "\n",
    "    t = get_filtered_list(description, split=False)\n",
    "    N_features = 10000\n",
    "    tfid_stop_vec = TfidfVectorizer(analyzer='word', max_df=0.9, stop_words='english'\n",
    "                                    , ngram_range=(1,3), max_features = N_features)\n",
    "    \n",
    "    x_tfid_stop_train = tfid_stop_vec.fit_transform(t)\n",
    "    sim = x_tfid_stop_train*x_tfid_stop_train.T\n",
    "    index = get_url_index(file)\n",
    "    url_list = get_url(file)\n",
    "    sim_index = find_similar_url(x_tfid_stop_train, index, url_list, url)\n",
    "    recommend = get_recommend_news(sim_index, file, index)\n",
    "    return recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def find_similar_matrix(tfidf_matrix, index_matrix, url, top_n = 30):\n",
    "    index = index_matrix[url]\n",
    "    cosine_similarities = linear_kernel(tfidf_matrix[index:index+1], tfidf_matrix).flatten()\n",
    "    related_docs_indices = [i for i in cosine_similarities.argsort()[::-1] if i != index]\n",
    "    return related_docs_indices[0:top_n]\n",
    "\n",
    "def get_date_sim(date_list, sim_date):\n",
    "    date_sim = []\n",
    "    for i in sim_date:\n",
    "        temp = date_list[i].replace('\\r\\n','')\n",
    "        date_sim.append(temp)\n",
    "    return date_sim\n",
    "\n",
    "def get_date_decay(date_sim):\n",
    "    today = datetime.datetime.now()\n",
    "    decay_list = []\n",
    "    for i in date_sim:\n",
    "        prev_day = datetime.datetime.strptime(i, '%Y_%m_%d')\n",
    "        res = today - prev_day\n",
    "        decay = exponential_decay(res.days)\n",
    "        decay_list.append(decay)\n",
    "    return decay_list\n",
    "\n",
    "def exponential_decay(t, init=1, m=30, finish=0.5):\n",
    "    alpha = np.log(init / finish) / m\n",
    "    l = - np.log(init) / alpha\n",
    "    decay = np.exp(-alpha * (t + l))\n",
    "    return decay\n",
    "\n",
    "def get_decay_matrix(original_matrix, id_list, decay_list):\n",
    "    for i in range(0, len(id_list)):\n",
    "        decay = decay_list[i]\n",
    "        decay_id = id_list[i]\n",
    "        original_matrix[decay_id] = original_matrix[decay_id]*decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decayed_recommendation_from_url(category, url):\n",
    "    df = convertFromRddtoDF(category)\n",
    "# def get_decayed_recommendation_from_url(df, url):\n",
    "    file =DF_reshape(df)\n",
    "    description = get_desc(file)\n",
    "    date = get_date(file)\n",
    "    \n",
    "    t = get_filtered_list(description, split=False)\n",
    "    N_features = 10000\n",
    "    tfid_stop_vec = TfidfVectorizer(analyzer='word', max_df=0.9, stop_words='english'\n",
    "                                    , ngram_range=(1,3), max_features = N_features)\n",
    "    \n",
    "    x_tfid_stop_train = tfid_stop_vec.fit_transform(t)\n",
    "    \n",
    "    sim = x_tfid_stop_train*x_tfid_stop_train.T\n",
    "    index = get_url_index(file)\n",
    "    url_list = get_url(file)\n",
    "    \n",
    "    sim_original = find_similar_matrix(x_tfid_stop_train, index, u)\n",
    "    sim_date = get_date_sim(date, sim_original)\n",
    "    decay_list = get_date_decay(sim_date)\n",
    "    get_decay_matrix(x_tfid_stop_train, sim_original, decay_list)\n",
    "    \n",
    "    sim_index = find_similar_url(x_tfid_stop_train, index, url_list, url)\n",
    "    recommend = get_recommend_news(sim_index, file, index)\n",
    "#     sim_index = find_similar_url(x_tfid_stop_train, index, url_list, url)\n",
    "#     recommend = get_recommend_news(sim_index, file, index)\n",
    "    return recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = 'https://www.nbcsports.com/bayarea/raiders/what-other-nfl-executives-thought-raiders-three-first-round-draft-picks'\n",
    "ca = 'sport'\n",
    "rec = get_recommendation_from_url(ca, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'date': '2019_04_25\\r\\n',\n",
       "  'url': 'https://panow.com/2019/04/24/the-raiders-are-back/'},\n",
       " {'date': '2019_04_26\\r\\n',\n",
       "  'url': 'https://www.silverandblackpride.com/2019/4/26/18517862/silver-mining-4-26-initial-grades-on-raiders-2019-nfl-draft-first-round-selections'},\n",
       " {'date': '2019_04_26\\r\\n',\n",
       "  'url': 'https://www.nbcsports.com/bayarea/raiders/nfl-draft-2019-raiders-pick-trayvon-mullen-no-40-add-secondary'},\n",
       " {'date': '2019_04_26\\r\\n',\n",
       "  'url': 'https://www.theguardian.com/sport/2019/apr/26/nfl-draft-giants-raiders'},\n",
       " {'date': '2019_04_25\\r\\n',\n",
       "  'url': 'https://www.denverbroncos.com/news/broncos-trade-back-in-first-round-of-nfl-draft-pick-up-extra-picks'},\n",
       " {'date': '2019_04_26\\r\\n',\n",
       "  'url': 'https://www.youtube.com/watch?v=pmNaaQrLNgQ'},\n",
       " {'date': '2019_04_26\\r\\n',\n",
       "  'url': 'https://www.youtube.com/watch?v=pmNaaQrLNgQ'},\n",
       " {'date': '2019_05_03\\r\\n',\n",
       "  'url': 'https://reignoftroy.com/2019/05/03/usc-football-four-trojans-2020-nfl-draft-first-rounders/'},\n",
       " {'date': '2019_04_26\\r\\n',\n",
       "  'url': 'https://www.cbsnews.com/news/nfl-draft-2019-picks-so-far-after-round-1-recap-2019-04-26/'},\n",
       " {'date': '2019_04_26\\r\\n',\n",
       "  'url': 'https://www.hogshaven.com/2019/4/26/18518235/welcome-to-the-second-round-of-the-2019-nfl-draft'}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'date': '2019_04_25\\r\\n',\n",
       "  'url': 'https://panow.com/2019/04/24/the-raiders-are-back/'},\n",
       " {'date': '2019_05_03\\r\\n',\n",
       "  'url': 'https://reignoftroy.com/2019/05/03/usc-football-four-trojans-2020-nfl-draft-first-rounders/'},\n",
       " {'date': '2019_04_26\\r\\n',\n",
       "  'url': 'https://www.silverandblackpride.com/2019/4/26/18517862/silver-mining-4-26-initial-grades-on-raiders-2019-nfl-draft-first-round-selections'},\n",
       " {'date': '2019_04_26\\r\\n',\n",
       "  'url': 'https://www.nbcsports.com/bayarea/raiders/nfl-draft-2019-raiders-pick-trayvon-mullen-no-40-add-secondary'},\n",
       " {'date': '2019_04_26\\r\\n',\n",
       "  'url': 'https://www.theguardian.com/sport/2019/apr/26/nfl-draft-giants-raiders'},\n",
       " {'date': '2019_04_25\\r\\n',\n",
       "  'url': 'https://www.denverbroncos.com/news/broncos-trade-back-in-first-round-of-nfl-draft-pick-up-extra-picks'},\n",
       " {'date': '2019_04_26\\r\\n',\n",
       "  'url': 'https://www.youtube.com/watch?v=pmNaaQrLNgQ'},\n",
       " {'date': '2019_04_26\\r\\n',\n",
       "  'url': 'https://www.youtube.com/watch?v=pmNaaQrLNgQ'},\n",
       " {'date': '2019_04_26\\r\\n',\n",
       "  'url': 'https://www.cbsnews.com/news/nfl-draft-2019-picks-so-far-after-round-1-recap-2019-04-26/'},\n",
       " {'date': '2019_04_26\\r\\n',\n",
       "  'url': 'https://www.thephinsider.com/2019/4/26/18518806/2019-nfl-draft-live-picks-update-miami-dolphins-rounds-2-3-chat-how-to-stream-watch-draft-order'}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
